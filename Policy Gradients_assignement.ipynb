{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HugoKD/RL/blob/main/RL_Assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTcTWPbRDXkm"
      },
      "source": [
        "# **Assignment 4** (114 pts)\n",
        "# Policy Gradients\n",
        "---\n",
        "---\n",
        "## **Instructions**\n",
        "* This is an individual assignment. You are **not allowed** to discuss the problems with other students.\n",
        "* Part of this assignment will be autograded by gradescope. You can use it as immediate feedback to improve your answers. You can resubmit as many times as you want.\n",
        "* All your solution, code, analysis, graphs, explanations should be done in this same notebook.\n",
        "* Please make sure to execute all the cells before you submit the notebook to the gradescope. You will not get points for the plots if they are not generated already.\n",
        "* Please **do not** change the random seeds\n",
        "* Please start early. Some of the experiments take a lot of time to run on CPU.\n",
        "* If you have questions regarding the assignment, you can ask for clarifications on Piazza. You should use the corresponding tag for this assignment.\n",
        "* The deadline for submitting this assignment is **10:00 PM on Sunday,** **November 26, 2023**\n",
        "---\n",
        "---\n",
        "\n",
        "This assignment has 4 parts. The goals of these parts are:\n",
        "- **Part 1**: Implementing a parameterized (neural network) policy with PyTorch\n",
        "- **Part 2**: Understanding and implementing the REINFORCE algorithm\n",
        "- **Part 3**: Extending the REINFORCE algorithm with a baseline\n",
        "- **Part 4**: Understanding and implementing Actor-Critic\n",
        "\n",
        "**When Submitting to GradeScope**: Be sure to\n",
        "1. Submit a .ipynb notebook to the Assignment 4 - Code section on Gradescope.\n",
        "2. Submit a pdf version of the notebook to the Assignment 4 - Report entry.\n",
        "\n",
        "Note: You can choose to submit responses in either English or French.\n",
        "\n",
        "Before starting the assignment, make sure that you have downloaded all the tests related\n",
        "for the assignment and put them in the appropriate locations. If you run the next cell,\n",
        "we will set this all up automatically for you in a dataset called public, which contains the test cases.\n",
        "\n",
        "### Installing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjKljKv4KeoM",
        "tags": [
          "otter_ignore"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f70f041-0389-45c0-f68a-f75855ac2141"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting otter-grader\n",
            "  Downloading otter_grader-5.2.2-py3-none-any.whl (157 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.0/158.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill (from otter-grader)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from otter-grader) (3.1.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from otter-grader) (5.9.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from otter-grader) (1.5.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from otter-grader) (6.0.1)\n",
            "Collecting python-on-whales (from otter-grader)\n",
            "  Downloading python_on_whales-0.67.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.1/159.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from otter-grader) (2.31.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from otter-grader) (1.14.1)\n",
            "Collecting jupytext (from otter-grader)\n",
            "  Downloading jupytext-1.15.2-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from otter-grader) (8.1.7)\n",
            "Collecting fica>=0.3.0 (from otter-grader)\n",
            "  Downloading fica-0.3.1-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from otter-grader) (7.34.0)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.10/dist-packages (from otter-grader) (1.6.3)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from otter-grader) (7.7.1)\n",
            "Collecting ipylab (from otter-grader)\n",
            "  Downloading ipylab-1.0.0-py3-none-any.whl (100 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.2/100.2 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from otter-grader) (6.5.4)\n",
            "Requirement already satisfied: docutils in /usr/local/lib/python3.10/dist-packages (from fica>=0.3.0->otter-grader) (0.18.1)\n",
            "Requirement already satisfied: sphinx in /usr/local/lib/python3.10/dist-packages (from fica>=0.3.0->otter-grader) (5.0.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse->otter-grader) (0.41.3)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from astunparse->otter-grader) (1.16.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->otter-grader) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->otter-grader) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->otter-grader) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->otter-grader) (3.6.6)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->otter-grader) (3.0.9)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->otter-grader)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader) (3.0.41)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader) (4.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->otter-grader) (2.1.3)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from jupytext->otter-grader) (0.10.2)\n",
            "Requirement already satisfied: markdown-it-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from jupytext->otter-grader) (3.0.0)\n",
            "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.10/dist-packages (from jupytext->otter-grader) (0.4.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (4.9.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (0.4)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (5.5.0)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (0.2.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (0.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (23.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->otter-grader) (2.19.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->otter-grader) (4.19.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->otter-grader) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->otter-grader) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->otter-grader) (1.23.5)\n",
            "Requirement already satisfied: pydantic!=2.0.*,<3,>=1.9 in /usr/local/lib/python3.10/dist-packages (from python-on-whales->otter-grader) (1.10.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from python-on-whales->otter-grader) (4.66.1)\n",
            "Requirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from python-on-whales->otter-grader) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-on-whales->otter-grader) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->otter-grader) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->otter-grader) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->otter-grader) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->otter-grader) (2023.7.22)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets->otter-grader) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets->otter-grader) (6.3.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->otter-grader) (0.8.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->otter-grader) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->otter-grader) (2023.11.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->otter-grader) (0.31.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->otter-grader) (0.13.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->otter-grader) (4.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=1.0.0->jupytext->otter-grader) (0.1.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->otter-grader) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->otter-grader) (0.2.10)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (6.5.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->otter-grader) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->otter-grader) (0.5.1)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.0->otter-grader) (1.0.7)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.0->otter-grader) (1.0.5)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.0->otter-grader) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.0->otter-grader) (2.0.4)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.0->otter-grader) (1.1.9)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.0->otter-grader) (1.0.6)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.0->otter-grader) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.0->otter-grader) (2.13.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.0->otter-grader) (0.7.13)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx->fica>=0.3.0->otter-grader) (1.4.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets->otter-grader) (23.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (23.1.0)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (1.5.8)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (0.18.0)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (0.18.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (1.0.0)\n",
            "Requirement already satisfied: jupyter-server>=1.8 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (1.24.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (0.2.3)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (21.2.0)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (1.6.4)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (1.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (1.1.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->otter-grader) (2.21)\n",
            "Installing collected packages: jedi, dill, python-on-whales, jupytext, ipylab, fica, otter-grader\n",
            "Successfully installed dill-0.3.7 fica-0.3.1 ipylab-1.0.0 jedi-0.19.1 jupytext-1.15.2 otter-grader-5.2.2 python-on-whales-0.67.0\n",
            "Cloning into 'public'...\n",
            "remote: Enumerating objects: 124, done.\u001b[K\n",
            "remote: Counting objects: 100% (124/124), done.\u001b[K\n",
            "remote: Compressing objects: 100% (76/76), done.\u001b[K\n",
            "remote: Total 124 (delta 53), reused 105 (delta 37), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (124/124), 1.03 MiB | 4.13 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n"
          ]
        }
      ],
      "source": [
        "!pip install otter-grader\n",
        "!git clone https://github.com/chandar-lab/INF8250ae-assignments-2023 public"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgJuZk9qrlsw",
        "tags": [
          "otter_ignore"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b00c6f8-f762-475a-f096-1b8357e01ece"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (2.5.2)\n",
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U pygame\n",
        "!pip install gymnasium\n",
        "!pip install matplotlib\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOdLGqFuwZXd",
        "tags": [
          "otter_ignore"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ae98e9f-4538-43ec-9c09-d73f42f067be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n"
          ]
        }
      ],
      "source": [
        "!apt-get install x11-utils > /dev/null 2>&1\n",
        "!pip install pyglet > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip install pillow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLC3E0lgJosu"
      },
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NrZRkF3rlsx",
        "tags": [
          "otter_ignore"
        ]
      },
      "outputs": [],
      "source": [
        "import otter\n",
        "\n",
        "grader = otter.Notebook(colab=True, tests_dir=\"./public/a4/tests\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAeKW1awDf9f"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.distributions as torchdist\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import warnings\n",
        "import functools\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "import gymnasium as gym\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from IPython.display import Image as IPImage, display\n",
        "\n",
        "device = \"cpu\"\n",
        "warnings.filterwarnings('ignore')\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGiAZ1fFrlsx"
      },
      "outputs": [],
      "source": [
        "GRADESCOPE_ENV_VAR = \"RUNNING_IN_GRADESCOPE\"\n",
        "\n",
        "\n",
        "def running_in_gradescope():\n",
        "    var = os.getenv(GRADESCOPE_ENV_VAR)\n",
        "    if var is None:\n",
        "        return False\n",
        "    return var == \"yes\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV1l7_Q2EMuy"
      },
      "source": [
        "# The Environment\n",
        "\n",
        "For this assignment, we will use  [`CartPole-v1`](https://gymnasium.farama.org/environments/classic_control/cart_pole/) from OpenAI Gymnasium. In this environment, the goal is to balance an inverted pendulum on a cart by moving the cart laterally. The state of the agent has four components:\n",
        "\n",
        "- The horizontal position of the cart, $x$\n",
        "- The velocity of the cart, $\\dot{x}$\n",
        "- The angle of the pendulum, measured relative to the vertical axis, $\\theta$\n",
        "- The angular velocity of the pendulum, $\\dot\\theta$\n",
        "\n",
        "There are two actions in the action space:\n",
        "- 0: push cart to the left\n",
        "- 1: push cart to the right\n",
        "\n",
        "The agent receives a reward of $1$ at each timestep, and the episode ends when the pendulum drops too far ($|\\theta|$ is more than $12^o$) or when the cart goes out of bounds. Also, the environment truncates after 500 steps if it hasn't already terminated, so the greatest possible return is $200$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn8e5oDkOIVQ"
      },
      "source": [
        "# Part 0. Video Rendering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxtNVy8Vv0h4"
      },
      "outputs": [],
      "source": [
        "def render_video(env, policy=None, steps=50):\n",
        "    env.action_space.seed(0)\n",
        "    obs, _ = env.reset(seed=0)\n",
        "    rewards = []\n",
        "    image_list = []\n",
        "    for i in range(steps):\n",
        "        if policy == None:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = policy.action(obs)\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        rewards.append(reward)\n",
        "        screen = env.render()\n",
        "        image_list.append(screen)\n",
        "\n",
        "        done = terminated or truncated\n",
        "        if done:\n",
        "            print(\"Return: \", sum(rewards))\n",
        "            break\n",
        "        env.close()\n",
        "    pil_images = [Image.fromarray(image) for image in image_list]\n",
        "    pil_images[0].save(\n",
        "        \"output.gif\", save_all=True, append_images=pil_images[1:], duration=50, loop=0\n",
        "    )\n",
        "    display(IPImage(\"output.gif\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Le1UW7LkOUmK"
      },
      "source": [
        "Now, let us see how a random policy performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raHs7Jj79zw9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "d4135078-3238-4364-9322-8f1ca2d9dfd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Return:  18.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/gif": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\", max_episode_steps=200)\n",
        "render_video(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7GsoCAqk57n"
      },
      "source": [
        "Notice that the random agent cannot balance the pendulum even for 20 time steps! In this assignment you will be implementing policy gradient algorithms to learn a better policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-Y49oxALyvq"
      },
      "source": [
        "# Part 1. Parameterized Policy Network (17 pts total)\n",
        "\n",
        "In this assignment, we will be studying Policy Gradient algorithms. In these algorithms, rather than using action-values to select actions, the policy itself is parameterized (in our case, by a neural network), and the policy is optimized directly via gradient ascent (although for practical purposes, we will be optimizing the negative of the objective via gradient descent).\n",
        "\n",
        "We will use a neural network to represent the policy here. The input to the neural network is a state, and the output should encode a probability distribution over the action space. Our environment has a discrete action space, so the policy should output parameters for a *Categorical distribution*.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b-HE0MVAoGc"
      },
      "source": [
        "## 1a: The Policy Network (5 pts)\n",
        "As a first step, fill in the `policy_init_network` function, which should return a torch neural net that will be to produce policy distributions for input states. You are free to experiment with different neural network architectures later, but for this assignment we recommend the following. Using `torch.nn.Sequential`, make a multilayer perceptron (MLP) with the following layers:\n",
        "\n",
        "1. A linear layer of size `(state space dimension, 32)`, followed by a ReLU activation\n",
        "1. A linear layer of size `(32, 32)`, followed by a ReLU activation\n",
        "1. A linear layer of size `(32, number of actions)` followed by a Softmax activation, to make a probability distribution over actions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFc7LOjmJejc",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def policy_init_network(env: gym.Env) -> nn.Module:\n",
        "    # Your code here\n",
        "    # ---------------------------------\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    num_actions = env.action_space.n\n",
        "    model = nn.Sequential(\n",
        "          nn.Linear(state_dim, 32),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(32, 32),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(32, num_actions),\n",
        "          nn.Softmax(dim = -1)\n",
        "        )\n",
        "    return model\n",
        "    # ---------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "xlfXvFsyXXHs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "outputId": "e8e48948-b06d-4458-c72e-de093e0a1a4f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "question 1a results: All test cases passed!"
            ],
            "text/html": [
              "<p><strong><pre style='display: inline;'>question 1a</pre></strong> passed! 🎉</p>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "grader.check(\"question 1a\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "-esPol7gSH2l"
      },
      "source": [
        "## 1b: The Policy Class (12 pts)\n",
        "In this part, we will build a class to represent the parameterized policy. This will be done in a few steps. In the constructor of the `Policy` class, initialize the variable `opt`, which will be used to optimize the policy parameters. This variable is a `torch.optim.Optimizer`. The learning rate is $10^{-3}$.\n",
        "\n",
        "### Part I: Optimizer (2 pts)\n",
        "Initialize the attribute `self.opt` to [Adam optimizer](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) with learning rate $10^{-3}$\n",
        "### Part II: The policy distribution (5 pts)\n",
        "Fill in the `dist` method. This method takes as input a state and outputs a torch [`Distribution`](https://pytorch.org/docs/stable/distributions.html) over actions.\n",
        "\n",
        "### Part III: Sampling actions (5 pts)\n",
        "Fill in the `action` method. This method samples an action from the policy given a state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmTbUIJ-PYAz",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Policy:\n",
        "    def __init__(\n",
        "        self, env: gym.Env, network: nn.Module, discount=0.99, name=\"Abstract Policy\"\n",
        "    ):\n",
        "        self.name = name\n",
        "        self.network = network\n",
        "        self.discount = discount\n",
        "\n",
        "        self.env = env\n",
        "        self.obs_dim = env.observation_space.shape[0]\n",
        "        self.n_actions = env.action_space.n\n",
        "        # Your code here (Part I)\n",
        "        # ========================\n",
        "        self.opt = torch.optim.Adam(params = self.network.parameters(), lr = 0.001)\n",
        "        # ========================\n",
        "\n",
        "    def distribution(self, x: np.ndarray) -> torchdist.distribution.Distribution:\n",
        "        \"\"\"\n",
        "        Get the distribution over actions for a given state. The distribution parameters\n",
        "        come from the network, which should output the unnormalized probabilities of\n",
        "        each action. (Note, this is not the same as unnormalized log probabilities).\n",
        "        \"\"\"\n",
        "        # Your code here (Part II)\n",
        "        # ========================\n",
        "        probs = self.network(torch.tensor(x, dtype=torch.float32)).unsqueeze(0)\n",
        "        dist = torchdist.Categorical(probs=probs)\n",
        "        # ========================\n",
        "        return dist\n",
        "\n",
        "    def action(self, x: np.ndarray) -> int:\n",
        "        \"\"\"\n",
        "        Sample an action from the policy at a given state\n",
        "\n",
        "        Input: a state encoded as a numpy array\n",
        "        Output: an action encoded as an int\n",
        "        \"\"\"\n",
        "        # Your code here (Part III)\n",
        "        # ========================\n",
        "        dist = self.distribution(x)\n",
        "        action = dist.sample().item()\n",
        "        # ========================\n",
        "        return action\n",
        "\n",
        "    def update(self, states, actions, rewards, dones) -> float:\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "M9qFWaSPXXHt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "outputId": "7e7392f9-8b5e-4b18-b46d-581a016b0c9b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "question 1b results: All test cases passed!"
            ],
            "text/html": [
              "<p><strong><pre style='display: inline;'>question 1b</pre></strong> passed! 🎉</p>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "grader.check(\"question 1b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNy4XmeFrlsy"
      },
      "source": [
        "## Generating Episode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdgVCpU8Savd"
      },
      "source": [
        "Now, the following function rolls out an episode in the environment with the policy. The function should return `(states, actions, rewards, terminated, truncated)` where\n",
        "\n",
        "1. `states` is a record of the states observed over the course of the episodes.\n",
        "1. `actions` is a record of the actions taken.\n",
        "1. `rewards` is a record of the rewards earned.\n",
        "1. `terminated` is an array of `bool`s that marks the termination of the episode.\n",
        "1. `truncated` is an array of `bool`s that marks the truncation of the episode.\n",
        "\n",
        "Note that in this function, we do not append the final state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cy1iw7lWSB4H"
      },
      "outputs": [],
      "source": [
        "def generate_episode(env: gym.Env, policy: Policy):\n",
        "    \"\"\"\n",
        "    Generates an episode given an environment and policy\n",
        "    Inputs:\n",
        "        env - Gymnasium environment\n",
        "        policy - policy for generating episode\n",
        "    Returns\n",
        "\n",
        "    \"\"\"\n",
        "    # Initialize lists\n",
        "    states = []\n",
        "    actions = []\n",
        "    rewards = []\n",
        "    terminated = []\n",
        "    truncated = []\n",
        "\n",
        "    # Reset environment\n",
        "    state, _ = env.reset(seed=0)\n",
        "    done = False\n",
        "\n",
        "    # Loop until end of episode\n",
        "    while not done:\n",
        "        states.append(state)\n",
        "        # Get action\n",
        "        action = policy.action(state)\n",
        "        actions.append(action)\n",
        "        # Take step\n",
        "        state, reward, term, trunc, _ = env.step(action)\n",
        "        done = term or trunc\n",
        "        rewards.append(reward)\n",
        "        terminated.append(term)\n",
        "        truncated.append(trunc)\n",
        "    states = np.array(states)\n",
        "    actions = np.array(actions)\n",
        "    rewards = np.array(rewards)\n",
        "    terminated = np.array(terminated)\n",
        "    truncated = np.array(truncated)\n",
        "    return (states, actions, rewards, terminated, truncated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCQ_nxFsrlsy"
      },
      "source": [
        "# Part 2. REINFORCE (17 pts total)\n",
        "In this section, you will implement the REINFORCE algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jocPdu0gdDTs"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "\n",
        "## 2a: Discounting Rewards (10 pts total)\n",
        "\n",
        "This problem has 2 parts.\n",
        "\n",
        "Recall the form of the REINFORCE policy gradient:\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta J(\\theta) = \\sum_{k=0}^T\\mathbf{E}\\left\\{G^{\\pi_\\theta}\\nabla_\\theta\\log\\pi_\\theta(a_k\\mid s_k)\\right\\}\n",
        "$$\n",
        "\n",
        "Here $\\pi_\\theta$ is the parameterized (neural net) policy with parameters $\\theta$, and $G^{\\pi_\\theta}$ is the random variable corresponding to the discounted return induced by following $\\pi_\\theta$. Note that at timestep $k$, action $a_k$ had no influence on rewards incurred before timestep $k$. For this reason, it is generally preferred to compute the following,\n",
        "\n",
        "$$\n",
        "\\widehat\\nabla_\\theta J(\\theta) = \\sum_{k=0}^T\\mathbf{E}\\left\\{G^{\\pi_\\theta}_k\\nabla_\\theta\\log\\pi_\\theta(a_k\\mid s_k)\\right\\}\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "G^{\\pi_\\theta}_k = \\sum_{t=k}^T\\gamma^{t-k}r(s_k, a_k)\n",
        "$$\n",
        "\n",
        "### Part I (5 pts):\n",
        "**Question**: Why do you think it is preferred to substitute $\\nabla_\\theta J(\\theta)$ for $\\widehat\\nabla_\\theta J(\\theta)$ in policy gradient algorithms?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htxLRto6rlsz"
      },
      "source": [
        "_Type your answer here, replacing this text._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sdb2afgwhPbW"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "### Part II (5 points):\n",
        "Implement the function `discounted_returns`, which computes the values $(G_1^{\\pi_\\theta},G_2^{\\pi_\\theta},\\dots)$ for a given sequence of rewards.\n",
        "\n",
        "The function takes three arguments:\n",
        "\n",
        "1. `rewards`: An array of rewards, which may have been collected over several trajectories.\n",
        "1. `dones`: An array of `bool`s, which mark where trajectories ended -- when either of `terminated` or `truncated` is `True`.\n",
        "1. `discount`: The discount factor.\n",
        "\n",
        "The output of the function should be a list of the same length as `rewards` containing the cumulative discounted future returns starting at each step in the reward sequence. Mathematically, for some index $k$, if $T$ is the first index after $k$ for which `dones[T] = True`, then\n",
        "\n",
        "$$\n",
        "\\texttt{returns}[k] = \\texttt{rewards}[k] + \\gamma\\texttt{rewards}[k+1] + \\dots + \\gamma^{T-k}\\texttt{rewards}[T]\\quad\n",
        "$$\n",
        "\n",
        "For example, suppose we gather data from two trajectories, which had rewards `[1,2,3]` and `[4, 2, 1]` respectively. Then:\n",
        "\n",
        "- `rewards = [1,2,3,4,2,1]`\n",
        "- `dones = [False, False, True, False, False, True]`\n",
        "\n",
        "For `discount = 0.5`, the output should be `[2.75, 3.5, 3, 5.25, 2.5, 1]`.\n",
        "\n",
        "**NOTE**: The output should be a numpy array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hoadj-naiOJk",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def discounted_returns(\n",
        "    rewards: np.ndarray, dones: np.ndarray, discount: float\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute discounted returns given rewards and terminateds\n",
        "    Inputs:\n",
        "        rewards - numpy array of reward values\n",
        "        dones - numpy array consisting of boolean values for whether the episode has terminated.\n",
        "        discount - discount factor\n",
        "    Returns:\n",
        "        returns - numpy array discounted returns\n",
        "    \"\"\"\n",
        "    # Your code here\n",
        "    # ===============================\n",
        "    returns = []\n",
        "    for i, reward in enumerate(reversed(rewards)):\n",
        "      if dones[len(rewards)- 1 - i]:\n",
        "        returns.append(reward)\n",
        "      else:\n",
        "        returns.append(reward + returns[-1] * discount)\n",
        "    # ===============================\n",
        "    return np.array(returns[::-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "5aqer_pkXXHu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "outputId": "c6c0e067-d730-4b90-9de3-28ce01a21654"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "question 2a results: All test cases passed!"
            ],
            "text/html": [
              "<p><strong><pre style='display: inline;'>question 2a</pre></strong> passed! 🌈</p>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "grader.check(\"question 2a\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TbcxxeokvSp"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "## 2b: The REINFORCE Update (7 pts)\n",
        "Finally, we'll implement REINFORCE. Fill in the `update` method for the `REINFORCEPolicy` class below. This method takes the following inputs:\n",
        "\n",
        "1. `states`: An array of observed states.\n",
        "1. `actions`: An array of actions taken at the corresponding `states`.\n",
        "1. `rewards`: An array of rewards received, where `rewards[k]` is the reward for taking actions `actions[k]` at state `states[k]`.\n",
        "1. `dones`: An array of `bool`s marking the end of episodes.\n",
        "\n",
        "This method should perform the following:\n",
        "- Compute the average policy gradient \"loss\", which is $-\\sum_{n=1}^{T}G_n^{\\pi_\\theta}\\log\\pi_\\theta(a_n\\mid s_n)$,  averaged over all trajectories\n",
        "- Compute the policy gradient\n",
        "- Update the policy parameters\n",
        "\n",
        "The method should return a dictionary that contains information from the update. For now, the dictionary should only have one entry with key `'policy_loss'` that contains a scalar loss from the policy gradient computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHIT-dd_k05l",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class REINFORCEPolicy(Policy):\n",
        "    def __init__(\n",
        "        self, env: gym.Env, network: nn.Module, discount=0.99, name=\"Plain REINFORCE\"\n",
        "    ):\n",
        "        super().__init__(env, network, discount=discount, name=name)\n",
        "\n",
        "    \"\"\"\n",
        "    Perform a gradient update\n",
        "    Inputs:\n",
        "        states, actions, rewards, dones: Output from generate_episode function\n",
        "    Returns:\n",
        "        Dictionary with the following keys:\n",
        "        - \"policy_loss\": float of the policy gradient loss (the quantity whose gradient is taken)\n",
        "                         This quantity should be the average of all losses that you compute.\n",
        "    \"\"\"\n",
        "\n",
        "    def update(self, states, actions, rewards, dones) -> float:\n",
        "        loss_dict = {}\n",
        "        # Your code here\n",
        "        # ======================\n",
        "\n",
        "        # Conversion required for dist.lob_prob\n",
        "        states = torch.tensor(states, dtype=torch.float32)\n",
        "        actions = torch.tensor(actions, dtype=torch.int64)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "\n",
        "        # Compute discounted rewards\n",
        "        G = discounted_returns(rewards, dones, self.discount)\n",
        "\n",
        "        # Compute policy logits and distribution\n",
        "        dist = self.distribution(states)\n",
        "\n",
        "        # Compute log probabilities of actions taken\n",
        "        log_probs = dist.log_prob(actions)\n",
        "\n",
        "        # Compute policy loss (negative average log probability weighted by returns)\n",
        "        policy_loss = -torch.mean(log_probs * torch.tensor(G))\n",
        "\n",
        "        # Zero the gradients, perform a backward pass, and update the weights\n",
        "        self.opt.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        self.opt.step()\n",
        "\n",
        "        loss_dict['policy_loss'] = policy_loss.item()\n",
        "        # ======================\n",
        "        return loss_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "h4iZpzZeXXHu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "outputId": "c644e3f3-87ff-4a29-8cfa-abe27ea10e6d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "question 2b results: All test cases passed!"
            ],
            "text/html": [
              "<p><strong><pre style='display: inline;'>question 2b</pre></strong> passed! 🎉</p>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "grader.check(\"question 2b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFJSefg4ig9k"
      },
      "source": [
        "# Part 3. REINFORCE with Baseline (41 pts total)\n",
        "When using a baseline in REINFORCE, the policy gradient formula is modified to the following,\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta J(\\theta) = \\sum_{k=0}^T\\mathbf{E}\\left\\{(G^{\\pi_\\theta} - b(s_k))\\nabla_\\theta\\log\\pi_\\theta(a_k\\mid s_k)\\right\\}\n",
        "$$\n",
        "\n",
        "for some function $b:S\\to\\mathbf{R}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsSzp_3zrls0"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "## 3a: Understanding the baseline (12 pts)\n",
        "1. **(3 pts)** What purpose does the baseline serve?\n",
        "1. **(3 pts)** If the baseline is a constant (that is, $b(s_1) = b(s_2)$ for any pair of states $(s_1, s_2)$), should we expect the performance of REINFORCE with this baseline to be any different from standard REINFORCE?\n",
        "1. **(3 pts)** Why can't the baseline be a function of the action as well as the state?\n",
        "1. **(3 pts)** Does the inclusion of an arbitrary baseline always help?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E62H18RdnyMK"
      },
      "source": [
        "\n",
        "## 3a: Answer\n",
        "1. The baseline reduces the variance of the gradient estimates, by centering the returns around an average value. This means that returns that are below the baseline and returns above it will lead to different signs when computing the gradient. This helps distinguishing genuinely good actions from bad ones.\n",
        "1. If the baseline is constant then the performance of REINFORCE is not expected to change. Indeed, substracting by a constant baseline will not affect the gradient updates.\n",
        "1. It can be a function of the actions but it will only bring unnecessary complexity to our function, the reason being that the action depends on the state, and it would not reduce the variance.\n",
        "1. No it does not, and choosing an adequate baseline is a challenge. Choosing one that is too low or too high can affect the learning and the performances quite drastically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLDPKzevkubU"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "## 3b: The Value Function (5 pts)\n",
        "In our experiments, we will use the value function as our baseline. It will be necessary to learn the value function from data, so our baseline will have the form\n",
        "\n",
        "$$\n",
        "b(s) = V^{\\pi_\\theta}_\\phi(s)\n",
        "$$\n",
        "\n",
        "where $\\phi$ denotes the parameters of the value function.\n",
        "\n",
        "Fill in the code for the construction of the value function neural net in `value_init_network`. The network architecture should be **similar** to that of the policy network besides the output layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMOJYE4rktzZ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def value_init_network(env: gym.Env) -> nn.Module:\n",
        "    # Your code here\n",
        "    # ===========================================================\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    model = nn.Sequential(\n",
        "          nn.Linear(state_dim, 32),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(32, 32),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(32, 1),\n",
        "        )\n",
        "    return model\n",
        "    # ==========================================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "XQkUoSReXXHz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 47
        },
        "outputId": "aab2975a-ae28-4c19-8496-45e4869e1f30"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "question 3b results: All test cases passed!"
            ],
            "text/html": [
              "<p><strong><pre style='display: inline;'>question 3b</pre></strong> passed! 🚀</p>"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "grader.check(\"question 3b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZU1lnKerls6"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "## 3c: REINFORCE with Baseline (9 pts)\n",
        "\n",
        "Fill in the constructor and the `update` method for `REINFORCEWithBaselinePolicy`.\n",
        "\n",
        "The constructor should do set two variables:\n",
        "* `self.value_network`: the value function neural network\n",
        "* `self.value_opt`: the `torch.optim.Optimizer` for the value function parameters. Use Adam optimizer and a learning rate of $2\\times 10^{-3}$ for the value optimizer.\n",
        "\n",
        "This method should perform the following:\n",
        "- Compute the \"policy gradient loss\", using the value predictions from the value function network instead of the Monte Carlo return estimates\n",
        "- Compute the policy gradient, again using the value predictions from the value function network instead of the Monte Carlo return estimates\n",
        "- Update the policy parameters\n",
        "- Compute the \"value loss\", which is mean squared difference between the Monte Carlo return estimates and the value function network predictions at each state in the trajectory\n",
        "- Update the value function network parameters\n",
        "\n",
        "As with the standard REINFORCE case, the `update` method returns a dictionary with a key `'policy_loss'` reflecting the loss w.r.t. the policy gradient objective. For `REINFORCEWithBaselinePolicy`'s `update` method, however, the dictionary should also have a key `'value_loss'` reflecting the loss w.r.t. the value function error."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class REINFORCEWithBaselinePolicy(Policy):\n",
        "    def __init__(\n",
        "        self,\n",
        "        env: gym.Env,\n",
        "        policy_network: nn.Module,\n",
        "        value_network: nn.Module,\n",
        "        discount=0.99,\n",
        "        name=\"REINFORCE with Baseline\",\n",
        "    ):\n",
        "        super().__init__(env, policy_network, discount=discount, name=name)\n",
        "        # Your code here\n",
        "        ## Initialize value network and optimizer\n",
        "        # ===========================================================\n",
        "        self.value_network = value_init_network(env)\n",
        "        self.value_opt = torch.optim.Adam(params = self.value_network.parameters(), lr = 0.002)\n",
        "        # ===========================================================\n",
        "\n",
        "    \"\"\"\n",
        "    Perform a gradient update\n",
        "    Inputs:\n",
        "        states, actions, rewards, dones: Output from rollout method\n",
        "    Returns:\n",
        "        Dictionary with the following keys:\n",
        "        - \"policy_loss\": float of the policy gradient loss (the quantity whose gradient is taken)\n",
        "        - \"value_loss\": float of the squared TD error\n",
        "    \"\"\"\n",
        "\n",
        "    def update(self, states, actions, rewards, dones) -> float:\n",
        "        loss_dict = {}\n",
        "        # Your code here\n",
        "        # ======================\n",
        "        # On calcule les returns G :\n",
        "        returns = discounted_returns(rewards, dones, self.discount)\n",
        "\n",
        "        # Distribution suivant les actions\n",
        "        dist = self.distribution(states)\n",
        "\n",
        "        # Convert data to PyTorch tensors\n",
        "        states = torch.tensor(states, dtype=torch.float32).to(device)\n",
        "        actions = torch.tensor(actions, dtype=torch.int64).to(device)\n",
        "        returns = torch.tensor(returns, dtype=torch.float32).to(device)\n",
        "\n",
        "        # On calcule les probas log de la policy\n",
        "        log_probs = dist.log_prob(actions)\n",
        "\n",
        "        # On calcule la baseline\n",
        "        baseline = self.value_network(states)\n",
        "\n",
        "        # Calcule la moyenne de la fonction de perte de la policy gradient avec baseline\n",
        "        policy_loss = -torch.mean(log_probs * (returns - baseline))\n",
        "\n",
        "        # Backpropagation + optimization\n",
        "        self.opt.zero_grad()\n",
        "        policy_loss.backward(retain_graph=True)\n",
        "        self.opt.step()\n",
        "\n",
        "        value_loss = F.mse_loss(baseline, returns)\n",
        "\n",
        "        # Backpropagation + optimization\n",
        "        self.value_opt.zero_grad()\n",
        "        value_loss.backward()\n",
        "        self.value_opt.step()\n",
        "\n",
        "        loss_dict[\"policy_loss\"] = float(policy_loss.item())\n",
        "        loss_dict[\"value_loss\"] = float(value_loss.item())\n",
        "\n",
        "        return loss_dict"
      ],
      "metadata": {
        "id": "seknEqKV1TKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "wurIPPvFXXHz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "bac90faa-ca3f-48c4-e6f3-3afe488c4e02"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "question 3c results:\n",
              "    question 3c - 1 result:\n",
              "        ✅ Test case passed\n",
              "\n",
              "    question 3c - 2 result:\n",
              "        ✅ Test case passed\n",
              "\n",
              "    question 3c - 3 result:\n",
              "        ❌ Test case failed\n",
              "        Error at line 97 in test question 3c:\n",
              "              assert_equal(loss[\"policy_loss\"], 4.115862846374512)\n",
              "        AssertionError: Scalars are not close!\n",
              "\n",
              "        Expected 4.115862846374512 but got 4.194655895233154.\n",
              "        Absolute difference: 0.07879304885864258 (up to 0 allowed)\n",
              "        Relative difference: 0.019143749876905647 (up to 0.001 allowed)"
            ],
            "text/html": [
              "<p><strong style='color: red;'><pre style='display: inline;'>question 3c</pre> results:</strong></p><p><strong><pre style='display: inline;'>question 3c - 1</pre> result:</strong></p><pre>    ✅ Test case passed</pre><p><strong><pre style='display: inline;'>question 3c - 2</pre> result:</strong></p><pre>    ✅ Test case passed</pre><p><strong><pre style='display: inline;'>question 3c - 3</pre> result:</strong></p><pre>    ❌ Test case failed\n",
              "    Error at line 97 in test question 3c:\n",
              "          assert_equal(loss[\"policy_loss\"], 4.115862846374512)\n",
              "    AssertionError: Scalars are not close!\n",
              "\n",
              "    Expected 4.115862846374512 but got 4.194655895233154.\n",
              "    Absolute difference: 0.07879304885864258 (up to 0 allowed)\n",
              "    Relative difference: 0.019143749876905647 (up to 0.001 allowed)</pre>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "grader.check(\"question 3c\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me_n0O1yvdbm"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "## 3d: Experiments (15 pts)\n",
        "\n",
        "The code below will train agents with REINFORCE with and without the value function baseline. Think about how you expect the return and loss curves to behave with and without the baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ATEh_JdvUIS",
        "tags": [
          "otter_ignore"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 964
        },
        "outputId": "7217e467-245a-4066-87d1-c479b80cc056"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Plain REINFORCE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 23%|██▎       | 113/500 [00:09<00:32, 11.84it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-aefe7003290e>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"value_loss\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mvalue_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrollout_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreinforce_policy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_runs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mstds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-aefe7003290e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"value_loss\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mvalue_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrollout_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreinforce_policy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_runs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mstds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-aefe7003290e>\u001b[0m in \u001b[0;36mrollout_score\u001b[0;34m(env, policy)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrollout_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-5c0c817d8a6a>\u001b[0m in \u001b[0;36mgenerate_episode\u001b[0;34m(env, policy)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Take step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterm\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_dot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_dot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         terminated = bool(\n\u001b[0m\u001b[1;32m    166\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1800x600 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\", max_episode_steps=200)\n",
        "agents = [\n",
        "    REINFORCEPolicy(env, policy_init_network(env)),\n",
        "    REINFORCEWithBaselinePolicy(env, policy_init_network(env), value_init_network(env)),\n",
        "]\n",
        "\n",
        "gradient_steps = 500\n",
        "scores = [np.zeros(gradient_steps) for _ in agents]\n",
        "stds = [np.zeros(gradient_steps) for _ in agents]\n",
        "\n",
        "test_runs = 5\n",
        "\n",
        "\n",
        "def rollout_score(env, policy):\n",
        "    _, _, rewards, _, _ = generate_episode(env, policy)\n",
        "    return np.sum(rewards)\n",
        "\n",
        "\n",
        "gs = list(range(gradient_steps))\n",
        "\n",
        "cmap = plt.get_cmap(\"viridis\")\n",
        "plt.figure()\n",
        "fig, (ret_ax, loss_ax, value_ax) = plt.subplots(nrows=1, ncols=3, figsize=(18, 6))\n",
        "\n",
        "value_losses = []\n",
        "\n",
        "if not running_in_gradescope():\n",
        "    for i in range(len(agents)):\n",
        "        reinforce_policy = agents[i]\n",
        "        print(f\"Training {reinforce_policy.name}\")\n",
        "        losses = []\n",
        "        for g in tqdm(range(gradient_steps)):\n",
        "            states, actions, rewards, terminated, truncated = generate_episode(\n",
        "                env, reinforce_policy\n",
        "            )\n",
        "            dones = [term or trunc for (term, trunc) in zip(terminated, truncated)]\n",
        "            loss = reinforce_policy.update(states, actions, rewards, dones)\n",
        "            losses.append(loss[\"policy_loss\"])\n",
        "            if \"value_loss\" in loss.keys():\n",
        "                value_losses.append(loss[\"value_loss\"])\n",
        "            res = [rollout_score(env, reinforce_policy) for _ in range(test_runs)]\n",
        "            scores[i][g] = np.mean(res)\n",
        "            stds[i][g] = np.std(res)\n",
        "        color = cmap(i / len(agents))\n",
        "        ret_ax.plot(gs, scores[i], label=reinforce_policy.name, color=color)\n",
        "        ret_ax.fill_between(\n",
        "            gs, scores[i] - stds[i], scores[i] + stds[i], alpha=0.3, color=color\n",
        "        )\n",
        "        loss_ax.plot(gs, losses, label=reinforce_policy.name, color=color)\n",
        "    ret_ax.legend()\n",
        "    ret_ax.grid(True)\n",
        "    ret_ax.margins(0)\n",
        "    ret_ax.set_title(\"Episode return\")\n",
        "    loss_ax.legend()\n",
        "    loss_ax.grid(True)\n",
        "    loss_ax.margins(0)\n",
        "    loss_ax.set_title(\"Policy loss\")\n",
        "    value_ax.plot(gs, value_losses, color=color)\n",
        "    value_ax.grid(True)\n",
        "    value_ax.margins(0)\n",
        "    value_ax.set_title(\"Value loss\")\n",
        "    plt.show()\n",
        "plt.close(\"all\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7K5HVAJOGVhW"
      },
      "source": [
        "### Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krY6hNw6IdOP"
      },
      "source": [
        "#### REINFORCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SNEBmOmGfSV",
        "tags": [
          "otter_ignore"
        ]
      },
      "outputs": [],
      "source": [
        "render_video(env, agents[0], steps=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bFjh9S9Ih3e"
      },
      "source": [
        "#### REINFORCE with Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BtqlNTUHbsL",
        "tags": [
          "otter_ignore"
        ]
      },
      "outputs": [],
      "source": [
        "render_video(env, agents[1], steps=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4-9zM3Urls7"
      },
      "source": [
        "### Analysis (5pts)\n",
        "In your experiments, how did the use of the value function baseline affect your results? Explain the results you observed. Also, observe the visualizations above and qualitatively comment the nature of the policies obtained from the two agents (i.e., with and without baseline)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY0UTzB-rls7"
      },
      "source": [
        "*   Without the baseline, the final return is 31, which is a poor result. Looking at the graphs, it seems that the agent is not really learning: the loss is not decreasing and the returns are not increasing.\n",
        "*   With the baseline, the final return is 200 which is the maximum score is our study case. This time, we can clearly see that the agent is learning from the curves.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhRgHXlL4fyH"
      },
      "source": [
        "# 4. Actor-Critic (39 pts total)\n",
        "\n",
        "Finally, we will experiment with an *actor-critic* algorithm. Recall that the gradient rule for REINFORCE with the value function baseline has the following form,\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta J(\\theta) = \\sum_{k=0}^T\\mathbf{E}\\left\\{(G^{\\pi_\\theta} - V^{\\pi_\\theta}_\\phi(s_k))\\nabla_\\theta\\log\\pi_\\theta(a_k\\mid s_k)\\right\\}\n",
        "$$\n",
        "\n",
        "Note that\n",
        "\n",
        "$$\n",
        "\\mathbf{E}\\left\\{G^{\\pi_\\theta}\\mid s_0 = s\\right\\} = \\mathbf{E}_{a\\sim\\pi(\\cdot\\mid s),s'\\sim P(\\cdot\\mid s, a)}\\left\\{r(s, a) + \\gamma V^{\\pi_\\theta}(s')\\right\\}\n",
        "$$\n",
        "\n",
        "Because of this, actor-critic algorithms estimate $G^{\\pi_\\theta}$ by $r(s, a) + \\gamma V^{\\pi_\\theta}(s')$. Thus, we can compute one gradient *per environment step*, since we no longer need data from the entire trajectory to estimate $G^{\\pi_\\theta}$. The gradient rule for the policy network (actor) is\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta J_{\\text{actor}}(\\theta) = \\mathbf{E}\\left\\{(r_k + \\gamma V^{\\pi_\\theta}_\\phi(s_{k+1}) - V^{\\pi_\\theta}_\\phi(s_k))\\nabla_\\theta\\log\\pi_\\theta(a_k\\mid s_k)\\right\\}\n",
        "$$\n",
        "\n",
        "for the policy parameters. The value network (critic) is trained to minimize the mean squared TD error:\n",
        "\n",
        "$$\n",
        "\\nabla_\\phi J_{\\text{critic}}(\\phi) = \\frac{1}{2}\\left(V^{\\pi_\\theta}_\\phi(s_k) - \\texttt{stop_gradient}\\left(r_k + \\gamma V^{\\pi_\\theta}_\\phi(s_{k+1})\\right)\\right)^2\n",
        "$$\n",
        "\n",
        "where $\\texttt{stop_gradient}$ enforces that no gradients flow through its argument."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-goKflXerls7"
      },
      "source": [
        "## 4a: Understanding Actor-Critic (15 pts total)\n",
        "\n",
        "This question is split into three conceptual questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mjrf55mBrls7"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "### Part I: Bias in Actor-Critic (5 pts)\n",
        "It is said that actor-critic policy gradients are more biased than REINFORCE policy gradients. Explain what this means. Are actor-critic policy gradients more biased than REINFORCE policy gradients computed with the value function baseline?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEQdfZ2xrls7"
      },
      "source": [
        "> The actor-critic method implies bootstrapping, as the value function is itself an estimate. If the value function is inaccurate, it introduces bias into the policy gradient updates (because errors in the value function estimate might happen).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3OUUvUKrls7"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "### Part II: Per-Step Updates (5 pts)\n",
        "Even though actor-critic algorithms can perform one update per step, the gradients are computed based on data from only one state transition as opposed to REINFORCE gradients which are averaged over $T$ state transitions. What is the benefit of updating once per environment step?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A6igDiarls7"
      },
      "source": [
        "> Updating every environment step means that the agent is constantly learning, which leads to constant and immediate adaptation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rqlImbErls7"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "### Part III: Lifelong Learning (5 pts)\n",
        "Imagine a scenario where an RL agent is to be deployed on a strange planet that we do not know how to simulate. Once we drop the robot on this planet, we can never interact with it again: it just autonomously learns from environment interactions for the rest of its life. Would you prefer to employ Actor-Critic or REINFORCE with baseline for this problem? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4-Y-i68rls7"
      },
      "source": [
        "> As explained in the question above, Actor-Critic allows immediate adaptation to the environment. It is thus much prefered to employ it to ensure that the robot will instantly learn about its environment. REINFORCE with baseline waits until the end of an episode, which might lead to a \"trial-and-error\" behaviour at first. This is definitely not something we would want from our agent.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBYhN3uU9CDk"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "## 4b: Implementing Actor-Critic (9 pts)\n",
        "Fill out the `ActorCriticPolicy` class below, according to the guidelines in the code. The `policy_init_network` and `value_init_network` methods will be used to instantiate the neural nets for the actor-critic, however they are trained differently in the actor-critic algorithm.\n",
        "\n",
        "Rather than implementing an `update` method for actor-critic, we will implement a method `train_episode` which rolls out an episode, performing updates at each step. More precisely, `train_episode` should do the following:\n",
        "\n",
        "1. Reset the environment to a starting state\n",
        "1. For each environment step:\n",
        "    1. Choose an action\n",
        "    1. Perform an environment step with the chosen action, observing the next state, reward, and terminal signal\n",
        "    1. Update both the actor and critic networks based on this transition\n",
        "1. Return a dictionary with the same entries as `REINFORCEWithBaselinePolicy`'s `update` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObWy7-gv9BRf",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class ActorCriticPolicy(Policy):\n",
        "    def __init__(\n",
        "        self,\n",
        "        env: gym.Env,\n",
        "        policy_network: nn.Module,\n",
        "        value_network: nn.Module,\n",
        "        discount=0.99,\n",
        "        name=\"Actor-Critic\",\n",
        "    ):\n",
        "        super().__init__(env, policy_network, discount=discount, name=name)\n",
        "        # Your code here\n",
        "        # Initialize self.value_network and self.value_opt like before\n",
        "        # ===========================================================\n",
        "        self.value_network = value_init_network(env)\n",
        "        self.value_opt = torch.optim.Adam(params = self.value_network.parameters(), lr = 0.002)\n",
        "        # ===========================================================\n",
        "\n",
        "    \"\"\"\n",
        "    Run a training episode\n",
        "    Inputs:\n",
        "        seed: Seed of the environment (default: 0)\n",
        "    Returns:\n",
        "        Dictionary with the following keys:\n",
        "        - \"policy_loss\": float of the policy gradient loss (the quantity whose gradient is taken),\n",
        "                        averaged over the episode\n",
        "        - \"value_loss\": float of the squared TD error averaged over the episode\n",
        "    \"\"\"\n",
        "\n",
        "    def train_episode(self, seed=0) -> float:\n",
        "        loss_dict = {}\n",
        "        state, _ = self.env.reset(seed=seed)\n",
        "        # Your code here\n",
        "        # ======================\n",
        "        done = False\n",
        "\n",
        "        policy_losses = []\n",
        "        value_losses = []\n",
        "\n",
        "        while not done:\n",
        "            action = self.action(state)\n",
        "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            current_state_value = self.value_network(torch.Tensor(state))\n",
        "            next_state_value = self.value_network(torch.Tensor(next_state))\n",
        "\n",
        "            delta = (reward + self.discount * next_state_value - current_state_value)\n",
        "            probs = self.distribution(state).log_prob(torch.tensor([action]))\n",
        "\n",
        "            loss_actor = -torch.mean(probs * delta)\n",
        "            policy_losses.append(loss_actor.item())\n",
        "\n",
        "            self.opt.zero_grad()\n",
        "            loss_actor.backward(retain_graph=True)\n",
        "            self.opt.step()\n",
        "\n",
        "            G = torch.tensor(reward + self.discount * next_state_value).detach()\n",
        "            loss_critic = F.mse_loss(current_state_value, G)/2\n",
        "            value_losses.append(loss_critic.item())\n",
        "\n",
        "            self.value_opt.zero_grad()\n",
        "            loss_critic.backward()\n",
        "            self.value_opt.step()\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "\n",
        "        loss_dict['policy_loss'] = np.mean(policy_losses)\n",
        "        loss_dict['value_loss'] = np.mean(value_losses)\n",
        "        # ======================\n",
        "        return loss_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "7xS9kb9AXXH1"
      },
      "outputs": [],
      "source": [
        "grader.check(\"question 4b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U41kVqmBKHC"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "## 4c: Experiments (15 pts)\n",
        "\n",
        "In the following experiments, we test the following agents:\n",
        "\n",
        "- REINFORCE with one trajectory per gradient update\n",
        "- REINFORCE with the value function baseline, one trajectory per gradient update\n",
        "- Actor-Critic\n",
        "\n",
        "Each agent is trained for 400 episodes, and the experiment is repeated 6 times with different random seeds. The plot displays the mean and variance of the return across the seeds for each agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8cqS3dZrls8",
        "tags": [
          "otter_ignore"
        ]
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\", max_episode_steps=200)\n",
        "import itertools\n",
        "\n",
        "SEEDS = [4, 8, 16, 23, 42]\n",
        "\n",
        "episodes = 500\n",
        "eval_runs = 5\n",
        "eval_every = 5\n",
        "epochs = list(range(0, episodes, eval_every))\n",
        "\n",
        "cmap = plt.get_cmap(\"viridis\")\n",
        "plt.grid(True)\n",
        "plt.margins(0)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Return\")\n",
        "\n",
        "pg_constructor = lambda: REINFORCEPolicy(env, policy_init_network(env))\n",
        "pg_baseline_constructor = lambda: REINFORCEWithBaselinePolicy(\n",
        "    env, policy_init_network(env), value_init_network(env)\n",
        ")\n",
        "\n",
        "ac_agent_constructor = lambda: ActorCriticPolicy(\n",
        "    env, policy_init_network(env), value_init_network(env)\n",
        ")\n",
        "\n",
        "if not running_in_gradescope():\n",
        "    ### ACTOR CRITIC\n",
        "    ac_agents = {}\n",
        "    ac_score_traces = {}\n",
        "    print(f\"Training Actor-Critic\")\n",
        "    for seed, ep in tqdm(itertools.product(SEEDS, np.arange(episodes))):\n",
        "        if seed not in ac_agents.keys():\n",
        "            np.random.seed(seed)\n",
        "            torch.manual_seed(seed)\n",
        "            ac_agents[seed] = ac_agent_constructor()\n",
        "            ac_score_traces[seed] = []\n",
        "            ac_agents[seed].env.action_space.seed(seed)\n",
        "        agent = ac_agents[seed]\n",
        "        loss = agent.train_episode()\n",
        "        if (ep + 1) % eval_every == 0:\n",
        "            res = [rollout_score(env, agent) for _ in range(eval_runs)]\n",
        "            ac_score_traces[seed].append(np.mean(res))\n",
        "\n",
        "    ac_data = np.vstack([ac_score_traces[seed] for seed in SEEDS])\n",
        "    ac_score_mean = np.mean(ac_data, axis=0)\n",
        "    ac_score_std = np.std(ac_data, axis=0)\n",
        "\n",
        "    plt.plot(epochs, ac_score_mean, color=cmap(0.8), label=\"Actor Critic\")\n",
        "    plt.fill_between(\n",
        "        epochs,\n",
        "        ac_score_mean - ac_score_std,\n",
        "        ac_score_mean + ac_score_std,\n",
        "        color=cmap(0.8),\n",
        "        alpha=0.3,\n",
        "    )\n",
        "\n",
        "    ### REINFORCE WITH BASELINE\n",
        "    pg_baseline_agents = {}\n",
        "    pg_baseline_score_traces = {}\n",
        "    print(f\"Training REINFORCE with Baseline\")\n",
        "    for seed, ep in tqdm(itertools.product(SEEDS, np.arange(episodes))):\n",
        "        if seed not in pg_baseline_agents.keys():\n",
        "            np.random.seed(seed)\n",
        "            torch.manual_seed(seed)\n",
        "            pg_baseline_agents[seed] = pg_baseline_constructor()\n",
        "            pg_baseline_score_traces[seed] = []\n",
        "            env.env.action_space.seed(seed)\n",
        "        agent = pg_baseline_agents[seed]\n",
        "        states, actions, rewards, terminated, truncated = generate_episode(env, agent)\n",
        "        dones = [term or trunc for (term, trunc) in zip(terminated, truncated)]\n",
        "        loss = agent.update(states, actions, rewards, dones)\n",
        "        if (ep + 1) % eval_every == 0:\n",
        "            res = [rollout_score(env, agent) for _ in range(eval_runs)]\n",
        "            pg_baseline_score_traces[seed].append(np.mean(res))\n",
        "\n",
        "    pg_baseline_data = np.vstack([pg_baseline_score_traces[seed] for seed in SEEDS])\n",
        "    pg_baseline_score_mean = np.mean(pg_baseline_data, axis=0)\n",
        "    pg_baseline_score_std = np.std(pg_baseline_data, axis=0)\n",
        "\n",
        "    plt.plot(\n",
        "        epochs, pg_baseline_score_mean, color=cmap(0.5), label=\"REINFORCE with Baseline\"\n",
        "    )\n",
        "    plt.fill_between(\n",
        "        epochs,\n",
        "        pg_baseline_score_mean - pg_baseline_score_std,\n",
        "        pg_baseline_score_mean + pg_baseline_score_std,\n",
        "        color=cmap(0.5),\n",
        "        alpha=0.3,\n",
        "    )\n",
        "\n",
        "    ### REINFORCE\n",
        "    pg_agents = {}\n",
        "    pg_score_traces = {}\n",
        "    print(f\"Training REINFORCE with Baseline\")\n",
        "    for seed, ep in tqdm(itertools.product(SEEDS, np.arange(episodes))):\n",
        "        if seed not in pg_agents.keys():\n",
        "            np.random.seed(seed)\n",
        "            torch.manual_seed(seed)\n",
        "            pg_agents[seed] = pg_constructor()\n",
        "            pg_score_traces[seed] = []\n",
        "            env.env.action_space.seed(seed)\n",
        "        agent = pg_agents[seed]\n",
        "        states, actions, rewards, terminated, truncated = generate_episode(env, agent)\n",
        "        dones = [term or trunc for (term, trunc) in zip(terminated, truncated)]\n",
        "        loss = agent.update(states, actions, rewards, dones)\n",
        "        if (ep + 1) % eval_every == 0:\n",
        "            res = [rollout_score(env, agent) for _ in range(eval_runs)]\n",
        "            pg_score_traces[seed].append(np.mean(res))\n",
        "\n",
        "    pg_data = np.vstack([pg_score_traces[seed] for seed in SEEDS])\n",
        "    pg_score_mean = np.mean(pg_data, axis=0)\n",
        "    pg_score_std = np.std(pg_data, axis=0)\n",
        "\n",
        "    plt.plot(epochs, pg_score_mean, color=cmap(0.2), label=\"REINFORCE\")\n",
        "    plt.fill_between(\n",
        "        epochs,\n",
        "        pg_score_mean - pg_score_std,\n",
        "        pg_score_mean + pg_score_std,\n",
        "        color=cmap(0.2),\n",
        "        alpha=0.3,\n",
        "    )\n",
        "\n",
        "    plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smA1VXUraGMu"
      },
      "source": [
        "### Visualizing AC policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPpZH-NuZmpx",
        "tags": [
          "otter_ignore"
        ]
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\", max_episode_steps=200)\n",
        "render_video(env, policy=ac_agents[4], steps=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8ROAYOprls8",
        "tags": [
          "otter_ignore"
        ]
      },
      "outputs": [],
      "source": [
        "plt.close(\"all\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "otter": {
      "OK_FORMAT": true,
      "tests": {
        "question 1a": {
          "name": "question 1a",
          "points": 5,
          "suites": [
            {
              "cases": [
                {
                  "code": ">>> test_env = gym.make('CartPole-v0')\n>>> test_output = policy_init_network(test_env)\n>>> test_state = torch.tensor([-0.04, 0.02, -0.04, 0.02])\n>>> test_out = test_output(test_state)\n>>> np.testing.assert_allclose(test_out.shape[0], 2)\n>>> np.testing.assert_allclose(test_out.detach().numpy().sum(), 1.0, rtol=1e-5, atol=0)\n",
                  "hidden": false,
                  "locked": false
                }
              ],
              "scored": true,
              "setup": "",
              "teardown": "",
              "type": "doctest"
            }
          ]
        },
        "question 1b": {
          "name": "question 1b",
          "points": [
            5,
            5,
            5
          ],
          "suites": [
            {
              "cases": [
                {
                  "code": ">>> test_state = env.reset()\n>>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_policy = Policy(env, policy_init_network(env), 0.99)\n>>> assert_equal(isinstance(test_policy.opt, torch.optim.Optimizer), True)\n",
                  "hidden": false,
                  "locked": false
                },
                {
                  "code": ">>> test_state = env.reset()\n>>> test_policy = Policy(env, policy_init_network(env), 0.99)\n>>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> assert_equal(isinstance(test_policy.dist(test_state).param_shape, torch.Size), True)\n>>> assert_equal(test_policy.dist(test_state).param_shape, torch.Size([1,2]))\n",
                  "hidden": false,
                  "locked": false
                },
                {
                  "code": ">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_state = env.reset()\n>>> test_policy = Policy(env, policy_init_network(env), 0.99)\n>>> assert_equal(isinstance(test_policy.dist(test_state), torch.distributions.Categorical), True)\n>>> assert_equal(test_policy.action(test_state) in [0,1], True)\n",
                  "hidden": true,
                  "locked": false
                }
              ],
              "scored": true,
              "setup": "",
              "teardown": "",
              "type": "doctest"
            }
          ]
        },
        "question 1c": {
          "name": "question 1c",
          "points": [
            3,
            3,
            4
          ],
          "suites": [
            {
              "cases": [
                {
                  "code": ">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_env = gym.make('CartPole-v0')\n>>> test_policy = Policy(test_env, policy_init_network(test_env), 0.99)\n>>> num_rollouts = 3\n>>> test_s, test_a, test_r, test_d = rollout(test_env, test_policy, num_rollouts)\n>>> test_len_rollout = test_s.shape[0]\n>>> assert_equal(isinstance(test_s, np.ndarray), True)\n>>> assert_equal(isinstance(test_a, np.ndarray), True)\n>>> assert_equal(isinstance(test_r, np.ndarray), True)\n>>> assert_equal(isinstance(test_d, np.ndarray), True)\n",
                  "hidden": false,
                  "locked": false
                },
                {
                  "code": ">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_env = gym.make('CartPole-v0')\n>>> test_policy = Policy(test_env, policy_init_network(test_env), 0.99)\n>>> num_rollouts = 3\n>>> test_s, test_a, test_r, test_d = rollout(test_env, test_policy, num_rollouts)\n>>> test_len_rollout = test_s.shape[0]\n>>> assert_equal(test_s.shape, (test_len_rollout, 4))\n>>> assert_equal(test_a.shape, (test_len_rollout,))\n>>> assert_equal(test_r.shape, (test_len_rollout,))\n>>> assert_equal(test_d.shape, (test_len_rollout,))\n",
                  "hidden": false,
                  "locked": false
                },
                {
                  "code": ">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_env = gym.make('CartPole-v0')\n>>> test_policy = Policy(test_env, policy_init_network(test_env), 0.99)\n>>> num_rollouts = 3\n>>> test_s, test_a, test_r, test_d = rollout(test_env, test_policy, num_rollouts)\n>>> assert_equal(np.any((test_a != 0)&(test_a != 1 )), False)\n>>> assert_equal(np.sum(test_d), num_rollouts)\n",
                  "hidden": true,
                  "locked": false
                }
              ],
              "scored": true,
              "setup": "",
              "teardown": "",
              "type": "doctest"
            }
          ]
        },
        "question 2a.3": {
          "name": "question 2a.3",
          "points": [
            2,
            2,
            2,
            3,
            3
          ],
          "suites": [
            {
              "cases": [
                {
                  "code": ">>> test_len_rollout = 5\n>>> test_d = np.array([True for _ in range(test_len_rollout)])\n>>> test_r = np.ones(test_len_rollout)\n>>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_ret = discounted_returns(test_r, test_d, 0.99)\n>>> assert_equal(test_ret.shape[0], test_len_rollout)\n",
                  "hidden": false,
                  "locked": false
                },
                {
                  "code": ">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=1e-5, atol=0)\n>>> test_rewards = [1.0,2.0,3.0,4.0,2.0,1.0]\n>>> test_dones = [False, False, True, False, False, True]\n>>> test_return = discounted_returns(test_rewards, test_dones, 0.5)\n>>> actual_return = np.array([2.75, 3.5, 3.0, 5.25, 2.5, 1.0])\n>>> assert_equal(test_return, actual_return)\n",
                  "hidden": false,
                  "locked": false
                },
                {
                  "code": ">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=1e-5, atol=0)\n>>> test_rewards = [1.0,2.0,3.0,4.0,2.0,1.0]\n>>> test_dones = [False, False, True, False, False, True]\n>>> test_return_perpetuity = discounted_returns(test_rewards, test_dones, 0.5, perpetuity=True)\n>>> actual_return_perpetuity = np.array([3.5, 5., 6., 5.5, 3., 2.])\n>>> assert_equal(test_return_perpetuity, actual_return_perpetuity)\n",
                  "hidden": false,
                  "locked": false
                },
                {
                  "code": ">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=1e-5, atol=0)\n>>> _other_discount = 0.8\n>>> _other_rewards = [2.0, 3.0, 1.0, 4.0, 5.0, 6.0]\n>>> _other_dones = [False, True, False, True, False, True]\n>>> _other_returns_true = np.array([4.4, 3.0, 4.2, 4.0, 9.8, 6.0])\n>>> _other_returns_pred = discounted_returns(_other_rewards, _other_dones, _other_discount)\n>>> assert_equal(_other_returns_pred, _other_returns_true)\n",
                  "hidden": true,
                  "locked": false
                },
                {
                  "code": ">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=1e-5, atol=0)\n>>> _other_discount = 0.8\n>>> _other_rewards = [2.0, 3.0, 1.0, 4.0, 5.0, 6.0]\n>>> _other_dones = [False, True, False, True, False, True]\n>>> _other_returns_perp_true = np.array([14., 15., 17., 20., 29., 30.])\n>>> _other_returns_perp_pred = discounted_returns(_other_rewards, _other_dones, _other_discount, perpetuity=True)\n>>> assert_equal(_other_returns_perp_pred, _other_returns_perp_true)\n",
                  "hidden": true,
                  "locked": false
                }
              ],
              "scored": true,
              "setup": "",
              "teardown": "",
              "type": "doctest"
            }
          ]
        },
        "question 2b": {
          "name": "question 2b",
          "points": 2,
          "suites": [
            {
              "cases": [
                {
                  "code": ">>> test_env = gym.make('CartPole-v0')\n>>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_reinforce = REINFORCEPolicy(test_env, policy_init_network(test_env), 0.99)\n>>> test_states, test_actions, test_rewards, test_dones = rollout(test_env, test_reinforce, 2)\n>>> loss = test_reinforce.update(test_states, test_actions, test_rewards, test_dones)\n>>> assert_equal(isinstance(loss,dict), True)\n>>> assert_equal('policy_loss' in loss, True)\n>>> assert_equal(isinstance(loss['policy_loss'], float), True)\n",
                  "hidden": false,
                  "locked": false
                }
              ],
              "scored": true,
              "setup": "",
              "teardown": "",
              "type": "doctest"
            }
          ]
        },
        "question 3b": {
          "name": "question 3b",
          "points": 3,
          "suites": [
            {
              "cases": [
                {
                  "code": ">>> test_env = gym.make('CartPole-v0')\n>>> test_state = test_env.reset()\n>>> test_value_net = value_init_network(test_env)\n>>> test_value = test_value_net(torch.FloatTensor(test_state))\n>>> np.testing.assert_allclose(test_value.shape[0], 1)\n",
                  "hidden": false,
                  "locked": false
                }
              ],
              "scored": true,
              "setup": "",
              "teardown": "",
              "type": "doctest"
            }
          ]
        },
        "question 3c": {
          "name": "question 3c",
          "points": [
            1,
            1,
            1
          ],
          "suites": [
            {
              "cases": [
                {
                  "code": ">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_env = gym.make('CartPole-v0')\n>>> test_value_net = value_init_network(test_env)\n>>> test_state = torch.tensor([-0.04, 0.02, -0.04, 0.02])\n>>> test_reinforce_baseline = REINFORCEWithBaselinePolicy(test_env, policy_init_network(test_env), test_value_net,0.99)\n>>> assert_equal(isinstance(test_reinforce_baseline.value_opt, torch.optim.Optimizer), True)\n",
                  "hidden": false,
                  "locked": false
                },
                {
                  "code": ">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_env = gym.make('CartPole-v0')\n>>> test_value_net = value_init_network(test_env)\n>>> test_state = torch.tensor([-0.04, 0.02, -0.04, 0.02])\n>>> test_reinforce_baseline = REINFORCEWithBaselinePolicy(test_env, policy_init_network(test_env), test_value_net,0.99)\n>>> test_states, test_actions, test_rewards, test_dones = rollout(test_env, test_reinforce_baseline, 2)\n>>> loss = test_reinforce_baseline.update(test_states, test_actions, test_rewards, test_dones)\n>>> assert_equal(isinstance(loss,dict), True)\n>>> assert_equal('policy_loss' in loss, True)\n",
                  "hidden": false,
                  "locked": false
                },
                {
                  "code": ">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_env = gym.make('CartPole-v0')\n>>> test_value_net = value_init_network(test_env)\n>>> test_state = torch.tensor([-0.04, 0.02, -0.04, 0.02])\n>>> test_reinforce_baseline = REINFORCEWithBaselinePolicy(test_env, policy_init_network(test_env), test_value_net,0.99)\n>>> test_states, test_actions, test_rewards, test_dones = rollout(test_env, test_reinforce_baseline, 2)\n>>> loss = test_reinforce_baseline.update(test_states, test_actions, test_rewards, test_dones)\n>>> assert_equal('value_loss' in loss, True)\n",
                  "hidden": false,
                  "locked": false
                }
              ],
              "scored": true,
              "setup": "",
              "teardown": "",
              "type": "doctest"
            }
          ]
        },
        "question 4b": {
          "name": "question 4b",
          "points": [
            2,
            3
          ],
          "suites": [
            {
              "cases": [
                {
                  "code": ">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_env = gym.make('CartPole-v0')\n>>> test_value_net = value_init_network(test_env)\n>>> test_actor_critic = ActorCriticPolicy(test_env, policy_init_network(test_env), test_value_net,0.99)\n>>> loss = test_actor_critic.train_episode()\n",
                  "hidden": false,
                  "locked": false
                },
                {
                  "code": ">>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> test_env = gym.make('CartPole-v0')\n>>> test_value_net = value_init_network(test_env)\n>>> test_actor_critic = ActorCriticPolicy(test_env, policy_init_network(test_env), test_value_net,0.99)\n>>> loss = test_actor_critic.train_episode()\n>>> assert_equal(isinstance(loss,dict), True)\n>>> assert_equal('policy_loss' in loss, True)\n>>> assert_equal('value_loss' in loss, True)\n",
                  "hidden": false,
                  "locked": false
                }
              ],
              "scored": true,
              "setup": "",
              "teardown": "",
              "type": "doctest"
            }
          ]
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
